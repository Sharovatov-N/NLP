{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ПЗ 10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Neural machine translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnxXKDjq3jEL"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfodePkj3jEa"
      },
      "source": [
        "## Download and prepare the dataset\n",
        "\n",
        "We'll use a language dataset provided by http://www.manythings.org/anki/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25lzkM166dfn",
        "outputId": "24e8d0a8-de48-4e0d-df8e-eb4b7349413b"
      },
      "source": [
        "!wget http://www.manythings.org/anki/fra-eng.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-28 13:35:47--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Распознаётся www.manythings.org (www.manythings.org)… 172.67.186.54, 104.21.92.44, 2606:4700:3030::6815:5c2c, ...\n",
            "Подключение к www.manythings.org (www.manythings.org)|172.67.186.54|:80... соединение установлено.\n",
            "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
            "Длина: 6451478 (6,2M) [application/zip]\n",
            "Сохранение в: «fra-eng.zip»\n",
            "\n",
            "fra-eng.zip         100%[===================>]   6,15M  3,71MB/s    за 1,7s    \n",
            "\n",
            "2021-07-28 13:35:49 (3,71 MB/s) - «fra-eng.zip» сохранён [6451478/6451478]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdYE7W-f6dfn",
        "outputId": "a8bd1e45-7f4d-4364-af6b-6b88b7bb5fc9"
      },
      "source": [
        "!mkdir fra-eng\n",
        "!unzip fra-eng.zip -d fra-eng/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  fra-eng.zip\n",
            "  inflating: fra-eng/_about.txt      \n",
            "  inflating: fra-eng/fra.txt         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRVATYOgJs1b"
      },
      "source": [
        "# Download the file\n",
        "\n",
        "path_to_file = './fra-eng/fra.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceDbLhql6dfo"
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    w = w.strip()\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yV9lZXQXNbnH",
        "outputId": "91c2ca2a-4e3a-4cb2-a4b6-57898c682af6"
      },
      "source": [
        "preprocess_sentence(\"I can't go.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> I can t go . <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHn4Dct23jEm"
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
        "\n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTbSbBz55QtF",
        "outputId": "68deb5e0-bf6c-4e8e-8cf9-f19a1ab4bb4e"
      },
      "source": [
        "en, ru = create_dataset(path_to_file, None)\n",
        "print(en[-1])\n",
        "print(ru[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> It may be impossible to get a completely error free corpus due to the nature of this kind of collaborative effort . However , if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning , we might be able to minimize errors . <end>\n",
            "<start> Il est peut tre impossible d obtenir un Corpus compl tement d nu de fautes , tant donn e la nature de ce type d entreprise collaborative . Cependant , si nous encourageons les membres produire des phrases dans leurs propres langues plut t que d exp rimenter dans les langues qu ils apprennent , nous pourrions tre en mesure de r duire les erreurs . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIOn8RCNDJXG"
      },
      "source": [
        "def tokenize(lang):\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAY9k49G3jE_"
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "    # creating cleaned input, output pairs\n",
        "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOi42V79Ydlr"
      },
      "source": [
        "### Limit the size of the dataset to experiment faster (optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8j9g9AnIeZV",
        "outputId": "07e6e03d-4a80-4419-c092-bedfaffd87b8"
      },
      "source": [
        "len(en), len(ru)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(190206, 190206)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnxC7q-j3jFD"
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 100000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QILQkOs3jFG",
        "outputId": "cd610938-12a5-4618-9ddd-a8777253e6ac"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "80000 80000 20000 20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJPmLZGMeD5q"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t!=0:\n",
        "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXukARTDd7MT",
        "outputId": "a3c58700-25d9-4093-cc2a-4cea4c26637b"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "4 ----> je\n",
            "75 ----> sais\n",
            "17 ----> que\n",
            "14 ----> tu\n",
            "50 ----> as\n",
            "221 ----> raison\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "4 ----> i\n",
            "42 ----> know\n",
            "5 ----> you\n",
            "19 ----> re\n",
            "94 ----> right\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqHsArVZ3jFS"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc6-NK1GtWQt",
        "outputId": "d49eb7f4-eed6-43fa-91e9-2a142698e60f"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 21]), TensorShape([64, 13]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=False,\n",
        "                                       return_state=False,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        state = self.gru(x, initial_state = hidden)\n",
        "        return state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60gSVh05Jl6l",
        "outputId": "3d84b8d3-5b59-457c-ac45-b35cc297d581"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "# print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ_B3mhW3jFk"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # used for attention\n",
        "        # self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x, initial_state=hidden)\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5UY8wko3jFp"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "decoder_sample_x, decoder_sample_h = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y0HF-zMF_vp",
        "outputId": "35e0f328-05a3-475d-8fe6-3961a429c117"
      },
      "source": [
        "decoder_sample_h.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 1024])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj8bXQTgNwrF"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC9ArXSsVfqn"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "\n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddefjBMa3jF0",
        "scrolled": true,
        "outputId": "c431b4f1-b28f-4266-ccf6-b172905843b9"
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 500 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                       batch,\n",
        "                                                       batch_loss.numpy()))\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.6165\n",
            "Epoch 1 Batch 500 Loss 1.5989\n",
            "Epoch 1 Batch 1000 Loss 1.2291\n",
            "Epoch 1 Loss 1.5658\n",
            "Time taken for 1 epoch 178.67937636375427 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.9695\n",
            "Epoch 2 Batch 500 Loss 0.9742\n",
            "Epoch 2 Batch 1000 Loss 0.7258\n",
            "Epoch 2 Loss 0.8769\n",
            "Time taken for 1 epoch 171.87355089187622 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.6072\n",
            "Epoch 3 Batch 500 Loss 0.5693\n",
            "Epoch 3 Batch 1000 Loss 0.4446\n",
            "Epoch 3 Loss 0.5256\n",
            "Time taken for 1 epoch 172.00407028198242 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.3294\n",
            "Epoch 4 Batch 500 Loss 0.2874\n",
            "Epoch 4 Batch 1000 Loss 0.3606\n",
            "Epoch 4 Loss 0.3212\n",
            "Time taken for 1 epoch 171.81873536109924 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1601\n",
            "Epoch 5 Batch 500 Loss 0.2945\n",
            "Epoch 5 Batch 1000 Loss 0.2432\n",
            "Epoch 5 Loss 0.2106\n",
            "Time taken for 1 epoch 171.73733758926392 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1168\n",
            "Epoch 6 Batch 500 Loss 0.1719\n",
            "Epoch 6 Batch 1000 Loss 0.1005\n",
            "Epoch 6 Loss 0.1514\n",
            "Time taken for 1 epoch 202.33160853385925 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0759\n",
            "Epoch 7 Batch 500 Loss 0.1060\n",
            "Epoch 7 Batch 1000 Loss 0.0959\n",
            "Epoch 7 Loss 0.1201\n",
            "Time taken for 1 epoch 172.26671051979065 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0831\n",
            "Epoch 8 Batch 500 Loss 0.0913\n",
            "Epoch 8 Batch 1000 Loss 0.1263\n",
            "Epoch 8 Loss 0.1034\n",
            "Time taken for 1 epoch 171.722900390625 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0973\n",
            "Epoch 9 Batch 500 Loss 0.0694\n",
            "Epoch 9 Batch 1000 Loss 0.0763\n",
            "Epoch 9 Loss 0.0918\n",
            "Time taken for 1 epoch 172.2716839313507 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0648\n",
            "Epoch 10 Batch 500 Loss 0.0718\n",
            "Epoch 10 Batch 1000 Loss 0.1092\n",
            "Epoch 10 Loss 0.0855\n",
            "Time taken for 1 epoch 172.09732747077942 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.0793\n",
            "Epoch 11 Batch 500 Loss 0.0966\n",
            "Epoch 11 Batch 1000 Loss 0.0918\n",
            "Epoch 11 Loss 0.0806\n",
            "Time taken for 1 epoch 171.6843020915985 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.0616\n",
            "Epoch 12 Batch 500 Loss 0.0846\n",
            "Epoch 12 Batch 1000 Loss 0.0969\n",
            "Epoch 12 Loss 0.0780\n",
            "Time taken for 1 epoch 172.3123540878296 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.0720\n",
            "Epoch 13 Batch 500 Loss 0.1149\n",
            "Epoch 13 Batch 1000 Loss 0.0687\n",
            "Epoch 13 Loss 0.0747\n",
            "Time taken for 1 epoch 171.94355249404907 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.0379\n",
            "Epoch 14 Batch 500 Loss 0.1130\n",
            "Epoch 14 Batch 1000 Loss 0.0569\n",
            "Epoch 14 Loss 0.0731\n",
            "Time taken for 1 epoch 173.9376573562622 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0669\n",
            "Epoch 15 Batch 500 Loss 0.0479\n",
            "Epoch 15 Batch 1000 Loss 0.1003\n",
            "Epoch 15 Loss 0.0714\n",
            "Time taken for 1 epoch 173.65255331993103 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.0441\n",
            "Epoch 16 Batch 500 Loss 0.0812\n",
            "Epoch 16 Batch 1000 Loss 0.0797\n",
            "Epoch 16 Loss 0.0703\n",
            "Time taken for 1 epoch 172.40438604354858 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0498\n",
            "Epoch 17 Batch 500 Loss 0.0653\n",
            "Epoch 17 Batch 1000 Loss 0.0520\n",
            "Epoch 17 Loss 0.0690\n",
            "Time taken for 1 epoch 171.37373065948486 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0579\n",
            "Epoch 18 Batch 500 Loss 0.0515\n",
            "Epoch 18 Batch 1000 Loss 0.0900\n",
            "Epoch 18 Loss 0.0680\n",
            "Time taken for 1 epoch 171.8741672039032 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.0588\n",
            "Epoch 19 Batch 500 Loss 0.0387\n",
            "Epoch 19 Batch 1000 Loss 0.0783\n",
            "Epoch 19 Loss 0.0682\n",
            "Time taken for 1 epoch 171.5653042793274 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0449\n",
            "Epoch 20 Batch 500 Loss 0.0302\n",
            "Epoch 20 Batch 1000 Loss 0.0572\n",
            "Epoch 20 Loss 0.0668\n",
            "Time taken for 1 epoch 173.43941926956177 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Translate\n",
        "\n",
        "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the *end token*.\n",
        "* And store the *attention weights for every time step*.\n",
        "\n",
        "Note: The encoder output is calculated only once for one input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbQpyYs13jF_"
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden = decoder(dec_input, dec_hidden)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl9zUHzg3jGI"
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJpT9D5_OgP6",
        "outputId": "d65a0803-025f-4810-f49d-4f71e4944528"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f09a45dd940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bhFfwcIMX5i",
        "outputId": "5cf5d1a2-0e5e-44e9-89c3-ed89fed9c7df"
      },
      "source": [
        "translate('où habites tu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> o habites tu <end>\n",
            "Predicted translation: where do you live ? <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSx2iM36EZQZ",
        "outputId": "d4cfe232-ef0b-4ab4-a0b2-110733aaa36c"
      },
      "source": [
        "translate('comment venir à paris ?')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> comment venir paris ? <end>\n",
            "Predicted translation: how did i lose my job ? <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3LLCx3ZE0Ls",
        "outputId": "b64aa087-8232-474e-e3c7-98c186081845"
      },
      "source": [
        "translate(u'essaye de le faire.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> essaye de le faire . <end>\n",
            "Predicted translation: try to do it . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUQVLVqUE1YW",
        "outputId": "b13a0911-5243-463c-ac2b-5381fe027c51"
      },
      "source": [
        "translate(u'comment se rendre au magasin')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> comment se rendre au magasin <end>\n",
            "Predicted translation: how s the going to work ? <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f09_hUFx9EJh",
        "outputId": "c23aaff7-a432-45f3-f15e-4cfa2b5d74a7"
      },
      "source": [
        "translate(u'combien ça coûte?')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> combien a co te ? <end>\n",
            "Predicted translation: how much is it ? <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7c5p8rmkHQG",
        "outputId": "d4682d71-f778-41f5-e4a9-2e1235976123"
      },
      "source": [
        "translate(u\"j'aime quand il neige.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> j aime quand il neige . <end>\n",
            "Predicted translation: i like it when snow . <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdXES85KkTVS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiqEJB206dfy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}